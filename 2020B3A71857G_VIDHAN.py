# -*- coding: utf-8 -*-
"""AI_assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nxMD4D6nNYcVNUjYif10N8qwsfkSkL9y
"""

import numpy as np
import matplotlib.pyplot as plt
import random
import math

class Bandit:
  #NOTE: the basic algorithm is explained in the first bandit algoritm function rest have just few variations specified explicitly
  #algorithm where the learning rate is 1/N and we can change the values of epsilon
    def bandit_Algo_N(self,epsilon,q):
      average_rewards = [0]*1000
      optimal_action = [0]*1000
      for j in range(2000):#2000 different random distributions
            Q = [q]*10 #optimistic initial value can be set by the user
            N = [0]*10
            samples=[]
            #generating new distributions 2000 times
            means = np.random.normal(0,1,10).tolist()
            for i in range(1000):#each distribution looped over 1000 timesteps
              x=random.random()#epsilon greedy action selection number
              if(x<=epsilon):
                #exploration
                A = random.randint(0,len(Q)-1)
              else:
                #greedy action selection
                A = Q.index(max(Q))
              R = np.random.normal(means[A],1)#choosing a value from the normal distribution of the Ath bandit
              N[A]=N[A]+1
              Q[A]= Q[A] + ((R - Q[A])/N[A])#updating the Q value based on the reward
              average_rewards[i]+=R #storing the rewards at each time step
              if(means.index(max(means))==A):
                optimal_action[i]+=1 # if the optimal action is chosen at that time step we increase the optimal action for that time step
      return average_rewards,optimal_action #returning the average rewards and the optimal action to user
    #algorithm where the learning rate is alpha (set by the user) and we can change the values of epsilon
    def bandit_Algo_alpha(self,epsilon,q,alpha):
      average_rewards = [0]*1000
      optimal_action = [0]*1000
      for j in range(2000):
            Q = [q]*10 #optimistic initial value can be set by the user
            N = [0]*10
            samples=[]
            means = np.random.normal(0,1,10).tolist()
            for i in range(1000):
              x=random.random()
              if(x<=epsilon):
                A = random.randint(0,len(Q)-1)
              else:
                A = Q.index(max(Q))
              R = np.random.normal(means[A],1)
              N[A]=N[A]+1
              Q[A]= Q[A] + alpha*((R - Q[A]))#learning rate is alpha here ( not 1/N)
              average_rewards[i]+=R
              if(means.index(max(means))==A):
                optimal_action[i]+=1
      return average_rewards,optimal_action
    #algorithm where the action selection is based on upper confidence bound action selection and rate of exploration is c
    def UCB(self,c,q,alpha):
      average_rewards = [0]*1000
      optimal_action = [0]*1000
      for j in range(2000):
            Q = [q]*10
            N = [0]*10
            samples=[]
            means = np.random.normal(0,1,10).tolist()
            for i in range(1000):
              x=random.random()
              for k in range(10):
                if(N[k]==0):
                  #if Nt is zero then the action is the maximal action
                  A = k
                  break
                else:
                  #Action selection based on upper confidence bound
                  A = k if (Q[k]+c*math.sqrt(math.log(i)/N[k]))>(Q[A]+c*math.sqrt(math.log(i)/N[A])) else A
              R = np.random.normal(means[A],1)
              N[A]=N[A]+1
              Q[A]= Q[A] + alpha*((R - Q[A]))
              average_rewards[i]+=R
              if(means.index(max(means))==A):
                optimal_action[i]+=1
      return average_rewards,optimal_action
    def __init__(self):
        print("EPSILON GREEDY")
      #part a epsilon greedy selection variation
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))
        epsilons=[0.1,0.01,0,0.05]
        for i,epsilon in enumerate(epsilons):
          #plotting the graph for bandit algo for different degrees of exploration
          average_rewards,optimal_action = self.bandit_Algo_N(epsilon,0) #calling the bandit algorithm where learning rate is 1/N
          ax1.plot(range(1,1001),np.array(average_rewards)/2000,label= epsilon)
          ax2.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= epsilon)
        average_rewards,optimal_action = self.bandit_Algo_alpha(epsilons[3],0,0.1) #calling the bandit algorithm where learning rate is alpha
        ax1.plot(range(1,1001),np.array(average_rewards)/2000,label= "alpha = 0.1 , epsilon = 0.05")
        ax2.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= "alpha = 0.1 , epsilon = 0.05")
        average_rewards,optimal_action = self.bandit_Algo_alpha(epsilons[3],0,0.05) #calling the bandit algorithm where learning rate is alpha
        ax1.plot(range(1,1001),np.array(average_rewards)/2000,label= "alpha = 0.05 , epsilon = 0.05")
        ax2.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label="alpha = 0.05 , epsilon = 0.05")
        ax1.set_ylabel('Average rewards')
        ax1.set_xlabel('Time steps')
        ax1.legend()
        ax2.set_ylabel('Optimal actions')
        ax2.set_xlabel('Time steps')
        ax2.legend()
        plt.show()

        print("OPIMISTIC INITIAL VALUE")
      # part b optimistic initial value
        fig, (ax3, ax4) = plt.subplots(2, 1, figsize=(6, 8))
        #plotting graph of bandit algorithm with varying initial values of Q and epislon
        #Q1=0,epsilon=0.1
        average_rewards,optimal_action = self.bandit_Algo_alpha(0.1,0,0.1)
        ax3.plot(range(1,1001),np.array(average_rewards)/2000,label= "Q=0,e=0.1")
        ax4.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= "Q=0,e=0.1")
        #Q1=0,epsilon=0.05
        average_rewards,optimal_action = self.bandit_Algo_alpha(0.05,0,0.1)
        ax3.plot(range(1,1001),np.array(average_rewards)/2000,label= "Q=0,e=0.05")
        ax4.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= "Q=0,e=0.05")
        #Q1=5,epsilon=0
        average_rewards,optimal_action = self.bandit_Algo_alpha(0,5,0.1)
        ax3.plot(range(1,1001),np.array(average_rewards)/2000,label= "Q=5,e=0")
        ax4.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= "Q=5,e=0")
        #Q1=3,epsilon=0
        average_rewards,optimal_action = self.bandit_Algo_alpha(0,3,0.1)
        ax3.plot(range(1,1001),np.array(average_rewards)/2000,label= "Q=3,e=0.1")
        ax4.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= "Q=3,e=0.1")
         #Q1=5,epsilon=0.1
        average_rewards,optimal_action = self.bandit_Algo_alpha(0.1,5,0.1)
        ax3.plot(range(1,1001),np.array(average_rewards)/2000,label= "Q=5,e=0.1")
        ax4.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= "Q=5,e=0.1")
        ax3.set_ylabel('Average rewards')
        ax3.set_xlabel('Time steps')
        ax3.legend()
        ax4.set_ylabel('Optimal actions')
        ax4.set_xlabel('Time steps')
        ax4.legend()
        plt.show()
        print("UCB")
    #part c upper confidence bound action selection
        cs = [0,1,2,3]
        fig, (ax5, ax6) = plt.subplots(2, 1, figsize=(6, 8))
        for i,c in enumerate(cs):
          #plotting graphs of average rewards and optimal action% with different degrees of exploration (c)
          average_rewards,optimal_action = self.UCB(c,0,0.1)
          ax5.plot(range(1,1001),np.array(average_rewards)/2000,label= c)
          ax6.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= c)
        #comparing the above graphs with the normal bandit algo where epsilon greedy approach is used
        average_rewards,optimal_action = self.bandit_Algo_alpha(0.1,0,0.1)
        ax5.plot(range(1,1001),np.array(average_rewards)/2000,label= "epsilon = 0.1")
        ax6.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= "epsilon =0.1")
        #comparing the above graphs with the normal bandit algo where epsilon greedy approach is used
        average_rewards,optimal_action = self.bandit_Algo_alpha(0.05,0,0.1)
        ax5.plot(range(1,1001),np.array(average_rewards)/2000,label= "epsilon = 0.05")
        ax6.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= "epsilon =0.05")
        #comparing the above graphs with the normal bandit algo where optimistic initial value  is used
        average_rewards,optimal_action = self.bandit_Algo_alpha(0,3,0.1)
        ax5.plot(range(1,1001),np.array(average_rewards)/2000,label= "epsilon = 0,Q=3")
        ax6.plot(range(1,1001),(np.array(optimal_action)/2000)*100,label= "epsilon =0,Q=3")
        ax5.set_ylabel('Average rewards')
        ax5.set_xlabel('Time steps')
        ax5.legend()
        ax6.set_ylabel('Optimal actions')
        ax6.set_xlabel('Time steps')
        ax6.legend()
        plt.show()
#end of question 1

class MRP:

  #NOTE: the basic algorithm is explained in the first temporal difference algo  function rest have just few variations specified explicitly
  #this function prints only the true value and values obtained in specific episodes
    def td_true_value(self,alpha):
        V= [0.5]*5 #initialising the values of states by 0.5
        episodes =100 #number of episodes to be run
        V_true = [1.0/6.0,2.0/6.0,3.0/6.0,4.0/6.0,5.0/6.0] #true values of each state a,b,c,d,e calculated mathematically
        plt.plot(['A','B','C','D','E'],V,label='Calculated value0',color='black',marker='o')
        rmse=[0]*100 #string the root mean squared error for each episode at each index
        for i in range(episodes):
          j=2 #starts from C state
          while(j>=0 and j < 5):# while state S is not terminal
            x=random.random()#henerating random number to choose the action
            s=j
            if(x<=0.5):
              j+=1#goes to right state if probability less than or equal to 0.5
            else:#goes to left state if probability greater than 0.5
              j-=1
            if(j==5):#if the right most terminal state is reached the reward is 1
              V[s]+=alpha*(1-V[s])#updating the values
            elif(j==-1):#if the left most terminal state is reached the reward is 0
              V[s]+=alpha*(-V[s])
            else:
              V[s]+=alpha*(0+V[j]-V[s])#reward is zero when non terminal state transition and updating V accordingly
          if(i==0):#plotting the value of V at episode 1
            plt.plot(['A','B','C','D','E'],V,label='Calculated value1',color='red',marker='o')
          if(i==9):#plotting the value of V at episode 10
            plt.plot(['A','B','C','D','E'],V,label='Calculated value10',color='green',marker='o')
          squared_errors = [(V_true[k] - V[k]) ** 2 for k in range(5)]
          mse = sum(squared_errors) / 5
          rmse[i] = np.sqrt(mse)
        #end of loop
        #plotting the value of V after 100 episodes
        plt.plot(['A','B','C','D','E'],V,label='Calculated value100',color='blue',marker='o')
        #plotting the original value
        plt.plot(['A','B','C','D','E'],V_true,label='Original value',color='black',marker='o')
        plt.ylabel('Values')
        plt.legend()
        plt.show()
        return rmse
    #in this function the learning rate is 1/n
    def td_rmse_N(self):
        V= [0.5]*5
        N=[0]*5
        episodes =100
        V_true = [1/6,2/6,3/6,4/6,5/6]
        rmse=[0]*100
        for i in range(episodes):
          j=2
          while(j>=0 and j < 5):
            x=random.random()
            s=j
            N[s]+=1
            if(x<=0.5):
              j+=1
            else:
              j-=1
            if(j==5):
              V[s]+=(1-V[s])/N[s]
            elif(j==-1):
              V[s]+=(-V[s])/N[s]
            else:
              V[s]+=(0+V[j]-V[s])/N[s]
          squared_errors = [(V_true[k] - V[k]) ** 2 for k in range(5)] #calculating  squared error for each state
          mse = sum(squared_errors) / 5  #calculating the mean
          rmse[i] = np.sqrt(mse) #storing the root of mean squared error in the specific episode index
        #end of loop
        return rmse #return the array
    #in this function the learning rate is determined by the values of alpha given by the user
    def td_rmse(self,alpha):
        V= [0.5]*5
        episodes =100
        V_true = [1/6,2/6,3/6,4/6,5/6]
        rmse=[0]*100
        for i in range(episodes):
          j=2
          while(j>=0 and j < 5):
            x=random.random()
            s=j
            if(x<=0.5):
              j+=1
            else:
              j-=1
            if(j==5):
              V[s]+=alpha*(1-V[s])#learning rate alpha
            elif(j==-1):
              V[s]+=alpha*(-V[s])
            else:
              V[s]+=alpha*(0+V[j]-V[s])
          squared_errors = [(V_true[k] - V[k]) ** 2 for k in range(5)]
          mse = sum(squared_errors) / 5
          rmse[i] = np.sqrt(mse)
        #end of loop
        return rmse
    def __init__(self):
        alphas =[0.05,0.1,0.15]
        rmse = self.td_true_value(0.1)#plotting true values with aplha = 0.1
        for i,alpha in enumerate(alphas):#plotting rmse for different values of alpha
          rmse =self.td_rmse(alpha)
          plt.plot(range(1,101),rmse,label=alpha)
        #plotting the rmse value for learning rate = 1/N
        rmse = self.td_rmse_N()
        plt.plot(range(1,101),rmse,label= 'N')
        plt.ylabel('errors')
        plt.xlabel('episodes')
        plt.legend()
        plt.show()

def main():
    print("Bandit")
    n=Bandit()
    print("Markov reward process")
    m=MRP()
if __name__=='__main__':
    main()